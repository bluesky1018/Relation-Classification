{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file_path = './files/train_attn_sp.txt'\n",
    "val_file_path = './files/val_attn_sp.txt'\n",
    "test_file_path = './files/test_attn_sp.txt'\n",
    "\n",
    "relation_file_path = './files/relations.txt'\n",
    "emb_google_txt = './word_embeddings/GoogleNews-vectors-negative300.txt'\n",
    "avg_vec_file = './word_embeddings/GoogleNews-vectors-negative300_avg_vec.txt'\n",
    "\n",
    "data_all_file_path = \"./data/data_all\"\n",
    "\n",
    "files = [train_file_path, val_file_path, test_file_path]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Other', 1: 'Message-Topic(e1,e2)', 2: 'Message-Topic(e2,e1)', 3: 'Product-Producer(e1,e2)', 4: 'Product-Producer(e2,e1)', 5: 'Instrument-Agency(e1,e2)', 6: 'Instrument-Agency(e2,e1)', 7: 'Entity-Destination(e1,e2)', 8: 'Entity-Destination(e2,e1)', 9: 'Cause-Effect(e1,e2)', 10: 'Cause-Effect(e2,e1)', 11: 'Component-Whole(e1,e2)', 12: 'Component-Whole(e2,e1)', 13: 'Entity-Origin(e1,e2)', 14: 'Entity-Origin(e2,e1)', 15: 'Member-Collection(e1,e2)', 16: 'Member-Collection(e2,e1)', 17: 'Content-Container(e1,e2)', 18: 'Content-Container(e2,e1)'}\n"
     ]
    }
   ],
   "source": [
    "label_to_int = {}\n",
    "int_to_label= {} \n",
    "\n",
    "with open(relation_file_path, 'r') as f:\n",
    "    for line in f: \n",
    "        line = line.strip().split()\n",
    "        key = str(line[0])\n",
    "        val = int(line[1])\n",
    "        label_to_int[key] = val \n",
    "        int_to_label[val] = key \n",
    "        \n",
    "print(int_to_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words_dataset) 25655\n",
      "./data/words_dataset.txt created\n"
     ]
    }
   ],
   "source": [
    "words_dataset = set()\n",
    "\n",
    "def create_words_dataset():\n",
    "    for f_name in files: \n",
    "        f = open(f_name, 'r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        lines = [ l.strip().split(\" \")[2:]  for l in lines]\n",
    "    \n",
    "        for line in lines:\n",
    "            for w in line:\n",
    "                words_dataset.add(w)\n",
    "    \n",
    "    print(\"len(words_dataset)\", len(words_dataset))\n",
    "    \n",
    "    with open('./data/words_dataset.txt', 'w') as f: \n",
    "        for w in sorted(words_dataset):\n",
    "            f.write(str(w) + \"\\n\")\n",
    "        print('./data/words_dataset.txt created')\n",
    "    \n",
    "    \n",
    "create_words_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "word_to_emb = {}\n",
    "\n",
    "with open(emb_google_txt, 'r', encoding='utf-8') as f: \n",
    "    first = True\n",
    "    \n",
    "    for line in f:\n",
    "        if first == True:\n",
    "            first = False\n",
    "            continue\n",
    "        line = line.strip().split()\n",
    "        if len(line) != 301:\n",
    "            continue \n",
    "        word = str(line[0])\n",
    "        vec = [float(x) for x in line[1:]]\n",
    "        vec = np.array(vec, dtype='float64')\n",
    "        \n",
    "        if word in words_dataset:\n",
    "            word_to_emb[word] = vec\n",
    "        elif word.lower() in words_dataset and word.lower() not in word_to_emb: \n",
    "            word_to_emb[word.lower()] = vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_vec.shape (300,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_avg_vec(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        line = f.readline()\n",
    "        line = line.strip().split()\n",
    "        line = [float(x) for x in line]\n",
    "        avg_vec = np.array(line, dtype='float64')\n",
    "        print(\"avg_vec.shape\", avg_vec.shape)\n",
    "        return avg_vec\n",
    "\n",
    "avg_vec = get_avg_vec(avg_vec_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_to_int) 25656\n",
      "embedding.shape (25656, 300)\n",
      "unknown_words 2652\n"
     ]
    }
   ],
   "source": [
    "word_to_int = {}\n",
    "embedding = []\n",
    "\n",
    "unknown_words = 0\n",
    "word_to_int['PADDING'] = len(word_to_int)\n",
    "embedding.append(np.zeros(300, dtype='float64'))\n",
    "\n",
    "for w in sorted(words_dataset): \n",
    "    word_to_int[w] = len(word_to_int)\n",
    "    if w in word_to_emb:\n",
    "        embedding.append(word_to_emb[w])\n",
    "    elif w.lower() in word_to_emb:\n",
    "        embedding.append(word_to_emb[w.lower()])\n",
    "    else:\n",
    "        unknown_words += 1\n",
    "        embedding.append(avg_vec)\n",
    "\n",
    "embedding = np.array(embedding, dtype='float64')\n",
    "print(\"len(word_to_int)\", len(word_to_int)) # 25656\n",
    "print(\"embedding.shape\", embedding.shape) # (25656, 300)\n",
    "print(\"unknown_words\", unknown_words) # 2652\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sent_len 101\n"
     ]
    }
   ],
   "source": [
    "def get_max_sent_len(files):\n",
    "    max_sent_len = 0 \n",
    "    for fname in files: \n",
    "        f = open(fname, 'r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        for l in lines: \n",
    "            l = l.strip().split(\" \")[2:]\n",
    "            max_sent_len = max(max_sent_len, len(l))\n",
    "    return max_sent_len\n",
    "\n",
    "max_sent_len = get_max_sent_len(files)\n",
    "print(\"max_sent_len\", max_sent_len) # 102-1 = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_matrices(file_name, word_to_int, label_to_int, max_sent_len):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    f = open(file_name, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    lines = [line.strip().split() for line in lines]\n",
    "    \n",
    "    for line in lines: \n",
    "        Y.append(label_to_int[line[1]])\n",
    "        line = line[2:]\n",
    "        tmp = np.zeros(max_sent_len, dtype='int32')\n",
    "        for i in range(len(line)):\n",
    "            tmp[i] = word_to_int[line[i]]\n",
    "        X.append(tmp)\n",
    "\n",
    "        \n",
    "    X = np.array(X, dtype='int32')\n",
    "    Y = np.array(Y, dtype='int32')\n",
    "    \n",
    "    return [X, Y]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = create_matrices(train_file_path, word_to_int, label_to_int, max_sent_len)\n",
    "val_set = create_matrices(val_file_path, word_to_int, label_to_int, max_sent_len)\n",
    "test_set = create_matrices(test_file_path, word_to_int, label_to_int, max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_all = [train_set, val_set, test_set, embedding, label_to_int, int_to_label]\n",
    "\n",
    "np.save(data_all_file_path, data_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
